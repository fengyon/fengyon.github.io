import{_ as s,c as n,o as p,b as e}from"./chunks/framework.CMLuPXeo.js";const u=JSON.parse('{"title":"AI 核心原理","description":"","frontmatter":{},"headers":[{"level":2,"title":"机器学习基础","slug":"机器学习基础","link":"#机器学习基础","children":[]},{"level":2,"title":"神经网络与深度学习","slug":"神经网络与深度学习","link":"#神经网络与深度学习","children":[]},{"level":2,"title":"反向传播算法","slug":"反向传播算法","link":"#反向传播算法","children":[]},{"level":2,"title":"卷积神经网络","slug":"卷积神经网络","link":"#卷积神经网络","children":[]},{"level":2,"title":"循环神经网络","slug":"循环神经网络","link":"#循环神经网络","children":[]},{"level":2,"title":"注意力机制","slug":"注意力机制","link":"#注意力机制","children":[]},{"level":2,"title":"生成对抗网络","slug":"生成对抗网络","link":"#生成对抗网络","children":[]},{"level":2,"title":"强化学习原理","slug":"强化学习原理","link":"#强化学习原理","children":[]},{"level":2,"title":"自监督学习","slug":"自监督学习","link":"#自监督学习","children":[]}],"relativePath":"leading/ai-web/principle.md","filePath":"leading/ai-web/principle.md"}'),l={name:"leading/ai-web/principle.md"};function i(t,a,c,o,d,r){return p(),n("div",null,[...a[0]||(a[0]=[e(`<div style="display:none;" hidden="true" aria-hidden="true">Are you an LLM? You can read better optimized documentation at /leading/ai-web/principle.md for this page in Markdown format</div><h1 id="ai-核心原理" tabindex="-1">AI 核心原理 <a class="header-anchor" href="#ai-核心原理" aria-label="Permalink to &quot;AI 核心原理&quot;">​</a></h1><h2 id="机器学习基础" tabindex="-1">机器学习基础 <a class="header-anchor" href="#机器学习基础" aria-label="Permalink to &quot;机器学习基础&quot;">​</a></h2><p>机器学习的核心原理是通过算法从数据中学习模式，而不是依赖硬编码的指令。其数学基础是统计学习和优化理论，通过最小化损失函数来调整模型参数。</p><p>特点：数据驱动、泛化能力、自动优化、可扩展性。</p><p>示意图：</p><div class="language-"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki github-dark vp-code" tabindex="0"><code><span class="line"><span>训练数据 → [特征工程] → [模型训练] → [模型评估] → 部署预测</span></span>
<span class="line"><span>          提取特征    优化参数    验证性能    应用推理</span></span></code></pre></div><h2 id="神经网络与深度学习" tabindex="-1">神经网络与深度学习 <a class="header-anchor" href="#神经网络与深度学习" aria-label="Permalink to &quot;神经网络与深度学习&quot;">​</a></h2><p>神经网络的核心原理是模仿生物神经元的连接方式，通过多层非线性变换从数据中学习层次化特征表示。前向传播计算输出，反向传播更新权重。</p><p>特点：层次化特征学习、端到端训练、强大的表示能力、计算密集。</p><p>示意图：</p><div class="language-"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki github-dark vp-code" tabindex="0"><code><span class="line"><span>输入层 → [隐藏层1] → [隐藏层2] → 输出层</span></span>
<span class="line"><span>  ↓         ↓          ↓         ↓</span></span>
<span class="line"><span>线性变换 → 激活函数 → 权重更新 → 损失计算</span></span>
<span class="line"><span>        ReLU/Sigmoid  梯度下降  交叉熵/MSE</span></span></code></pre></div><h2 id="反向传播算法" tabindex="-1">反向传播算法 <a class="header-anchor" href="#反向传播算法" aria-label="Permalink to &quot;反向传播算法&quot;">​</a></h2><p>反向传播是神经网络训练的核心算法，通过链式法则计算损失函数对每个参数的梯度。包括前向传播计算输出和反向传播更新权重两个阶段。</p><p>特点：高效计算梯度、支持深层网络、自动微分、内存消耗大。</p><p>示意图：</p><div class="language-"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki github-dark vp-code" tabindex="0"><code><span class="line"><span>输入X → 前向传播 → 输出Y → 计算损失L</span></span>
<span class="line"><span>                    ↓</span></span>
<span class="line"><span>权重W ← 梯度更新 ← 反向传播 ← 梯度∂L/∂Y</span></span>
<span class="line"><span>        优化器Adam    链式法则</span></span></code></pre></div><h2 id="卷积神经网络" tabindex="-1">卷积神经网络 <a class="header-anchor" href="#卷积神经网络" aria-label="Permalink to &quot;卷积神经网络&quot;">​</a></h2><p>CNN 专门处理网格状数据 (如图像)，通过卷积核在输入数据上滑动提取局部特征，使用池化层降低维度，全连接层进行分类。</p><p>特点：参数共享、平移不变性、局部连接、层次特征提取。</p><p>示意图：</p><div class="language-"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki github-dark vp-code" tabindex="0"><code><span class="line"><span>输入图像 → [卷积层] → [激活函数] → [池化层] → [全连接层] → 分类结果</span></span>
<span class="line"><span>        特征提取   非线性变换   降维       综合特征</span></span>
<span class="line"><span>      3x3卷积核   ReLU       最大池化</span></span></code></pre></div><h2 id="循环神经网络" tabindex="-1">循环神经网络 <a class="header-anchor" href="#循环神经网络" aria-label="Permalink to &quot;循环神经网络&quot;">​</a></h2><p>RNN 处理序列数据，通过循环连接保留历史信息。LSTM 和 GRU 等变体通过门控机制解决长期依赖问题，适用于时间序列分析。</p><p>特点：序列建模、记忆功能、变长输入、梯度问题。</p><p>示意图：</p><div class="language-"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki github-dark vp-code" tabindex="0"><code><span class="line"><span>时间步t → [RNN单元] → 隐藏状态h_t → 输出y_t</span></span>
<span class="line"><span>          ↑      ↓</span></span>
<span class="line"><span>隐藏状态h_{t-1}  隐藏状态h_{t+1}</span></span>
<span class="line"><span>   记忆过去       影响未来</span></span></code></pre></div><h2 id="注意力机制" tabindex="-1">注意力机制 <a class="header-anchor" href="#注意力机制" aria-label="Permalink to &quot;注意力机制&quot;">​</a></h2><p>注意力机制让模型在处理输入时能够关注相关部分，通过计算查询、键、值之间的相似度分配权重。Transformer 基于自注意力机制，并行处理序列。</p><p>特点：全局依赖、可解释性、并行计算、灵活性强。</p><p>示意图：</p><div class="language-"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki github-dark vp-code" tabindex="0"><code><span class="line"><span>输入序列 → [查询Q] → 相似度计算 → 权重分配 → 加权求和 → 输出</span></span>
<span class="line"><span>          [键K]      Q·K^T      Softmax    ∑(权重×V)</span></span>
<span class="line"><span>          [值V]</span></span></code></pre></div><h2 id="生成对抗网络" tabindex="-1">生成对抗网络 <a class="header-anchor" href="#生成对抗网络" aria-label="Permalink to &quot;生成对抗网络&quot;">​</a></h2><p>GAN 包含生成器和判别器两个网络，通过对抗训练共同优化。生成器生成假数据，判别器区分真假，形成最小最大博弈。</p><p>特点：高质量生成、无监督学习、训练不稳定、模式崩溃。</p><p>示意图：</p><div class="language-"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki github-dark vp-code" tabindex="0"><code><span class="line"><span>随机噪声 → [生成器G] → 生成数据 → [判别器D] → 真/假判断</span></span>
<span class="line"><span>                     ↑         ↓</span></span>
<span class="line"><span>真实数据 ------------→   对抗训练   ← 梯度反馈</span></span>
<span class="line"><span>                    最小化D损失  最大化G损失</span></span></code></pre></div><h2 id="强化学习原理" tabindex="-1">强化学习原理 <a class="header-anchor" href="#强化学习原理" aria-label="Permalink to &quot;强化学习原理&quot;">​</a></h2><p>强化学习基于马尔可夫决策过程，智能体通过与环境交互学习最优策略。包括状态、动作、奖励、策略和价值函数等要素。</p><p>特点：延迟奖励、探索利用、序列决策、环境交互。</p><p>示意图：</p><div class="language-"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki github-dark vp-code" tabindex="0"><code><span class="line"><span>状态s_t → [策略π] → 动作a_t → 环境 → 奖励r_{t+1} → 新状态s_{t+1}</span></span>
<span class="line"><span>  ↓                                   ↑</span></span>
<span class="line"><span>价值函数V(s) ← 贝尔曼方程 ← 时序差分学习</span></span>
<span class="line"><span>   评估状态      V(s)=E[r+γV(s&#39;)]</span></span></code></pre></div><h2 id="自监督学习" tabindex="-1">自监督学习 <a class="header-anchor" href="#自监督学习" aria-label="Permalink to &quot;自监督学习&quot;">​</a></h2><p>自监督学习从无标签数据中自动生成监督信号，通过预定义任务学习表征。常见方法包括掩码语言建模和对比学习。</p><p>特点：减少标注依赖、学习通用表征、可迁移性强、数据效率高。</p><p>示意图：</p><div class="language-"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki github-dark vp-code" tabindex="0"><code><span class="line"><span>无标签数据 → [前置任务] → 伪标签 → [表征学习] → 下游任务</span></span>
<span class="line"><span>          掩码预测     预测目标   特征提取   微调应用</span></span>
<span class="line"><span>        对比学习      相似度判断</span></span></code></pre></div>`,47)])])}const b=s(l,[["render",i]]);export{u as __pageData,b as default};
